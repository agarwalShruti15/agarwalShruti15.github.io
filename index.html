
<!DOCTYPE html>
<html lang="en">
  <head>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156531061-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-156531061-1');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Shruti Agarwal</title>

    <!-- Bootstrap core CSS -->
    <link href="bootstrap.css" rel="stylesheet">
    <!-- Custom styles for this template -->
  </head>

  <body>

  <div class="container">
    <div class="page-header">
      <table border="0" cellpadding="0" cellspacing="0" width="100%">
        <td  valign="top" align="left" width="18%">
          <a href="img/IMG_9290.JPG">
          <img src="img/IMG_9290_cropped.png" border=0 height=180></a>
        </td>
        <td align="left">
        <h1>Shruti Agarwal</h1>
        <p style="text-align: left;">
              <span style="font-family:georgia,serif;"><span style="font-size: 12pt;">Research Scientist<br/>
                Content Authenticity Initiative<br/>
                Adobe<br/>
              <br />
              Email: shragarw[at] adobe [dot] com </span></span></p>
        </td>
      </table>
    </div>

  <div style="color:#676767;font-family:Arial, Helvetica, sans-serif;font-size:9px;">
  <p><h3>
      <a href="#about"">About</a> | <a href="#innews"> News</a> | <a href="#teaching">Teaching</a> | <a href="#research">Research</a> 
    </h3>
  </div>
  <p><hr size="1" align="left" noshade><p>

<p><a name="about"></a> I’m a research scientist in AI for Content Authenticity team at Adobe. My primary research interest lies in content provenance and multimedia forensics with a focus on both active and passive forensic techniques like watermarking, deep-fake detection, robust image fingerprint, etc. 
Previously, I was a postdoc at CSAIL with Prof. William Freeman. Simultaneously, I was also working as a Lecturer for Computer Vision course in MIDS program at Berkeley School of Information. I did my PhD from CS department of UC Berkeley advised by <a href="https://farid.berkeley.edu">Prof. Hany Farid</a>.</p>
<p>
  <a href="https://www.dropbox.com/s/xgyuczxspstny5s/CV.pdf?dl=0">CV</a> (Last updated 2022), 
  <a href="https://github.com/agarwalShruti15">GitHub</a>, <a href="https://scholar.google.com/citations?user=gcuoIb0AAAAJ&hl=en">Google Scholar</a>, <a href="https://www.linkedin.com/in/shruti-agarwal-54763870/">LinkedIn</a>.
</p>

<h3><p><a name="innews"></a>News</h3>

  <ul>
    <LI>
      Keynote Speaker at Workshop On Media Forensics, CVPR, Louisiana, 06.19.22 [<a href="https://sites.google.com/view/mediaforensics2022/invited-speakers?authuser=0">Workshop</a>], [<a href="https://www.dropbox.com/s/8aty2ihhskuento/multi-modal_deepfake_detection.pdf?dl=0">ppt</a>]
    </LI>
    <LI>
      `Robert Pattinson' TikTok account is latest unlikely celebrity profile raising questions, 05.24.22 [<a href="https://www.nbcnews.com/pop-culture/viral/robert-pattinson-tiktok-account-latest-unlikely-celebrity-profile-rais-rcna25217">read</a>]
    </LI>
      <LI>
        Moments of untruth: Using technology to expose digital deception, Berkeley Engineering, 10.15.19 [<a href="https://engineering.berkeley.edu/magazine/fall-2019/moments-untruth">read</a>]
      </LI>
      <LI>
        Race to defuse deepfake videos: UC Berkeley researchers creating software for newsrooms, abc7, 07.08.19 [<a href="https://abc7news.com/technology/uc-berkeley-researchers-creating-software-to-defuse-deepfake-videos/5383400/">read</a>]
      </LI>
      <LI>
        UC Berkeley researchers develop technique for detecting AI video simulations, The Daily California, 07.05.19 [<a href="https://www.dailycal.org/2019/07/05/uc-berkeley-researchers-develop-technique-for-detecting-ai-video-simulations/">read</a>]
      </LI>
      <LI>
        The fight to stay ahead of deepfake videos before the 2020 US election, CNN Business, 06.12.19 [<a href="https://www.cnn.com/2019/06/12/tech/deepfake-2020-detection/index.html">read</a>]
      </LI>
      <LI>
        Researchers use facial quirks to unmask ’deepfakes’, Berkeley News, 06.18.19 [<a href="https://news.berkeley.edu/2019/06/18/researchers-use-facial-quirks-to-unmask-deepfakes/">read</a>]
      </LI>
    </ul>

  <h3><a name="Teaching"></a>Teaching</h3>

  <ul>
    <LI>
      <a href="https://www.ischool.berkeley.edu/courses/datasci/281">Data Science W281, Computer Vision</a> (Spring, Summer, Fall '22)
    </LI>
  </ul>

  <h3><a name="research"></a>Research</h3>
    <table border="0" cellpadding="0" cellspacing="0" width="100%">

      <tr>
        <td  valign="top" align="center">
          <img src="projects/arxiv_21.png" border=0 height=360></a>
        </td>
        <td valign="top" align="left">
          <ul>
            <LI>
              <b> Deep Fake Detection:  </b> 
              <BR>
                The creation of sophisticated fake videos has been largely relegated to Hollywood
                studios or state actors. Recent advances in deep learning, however, have democratized
                the creation of sophisticated and compelling fake images, videos, and audios.
                This synthetically-generated media – so-called deep fakes – continue to capture the
                imagination of the computer-graphics and computer-vision communities. At the
                same time, the easy access to technology that can create deep fakes of anybody
                saying anything continues to be of concern because of its power to disrupt democratic
                elections, commit small to large-scale fraud, fuel dis- and mis-information
                campaigns, and create non-consensual pornography.
              <p>
              <ul>
                <LI>
                  <b> "Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion.", </b>
                  <BR>
                    <b> S. Agarwal </b>, L. Hu, E. Ng, T. Darrell, H. Li, and A. Rohrbach,
                  <BR>
                  <em>WACV, 2023. </em> 
                  <a href="https://www.dropbox.com/s/gk4ppwjuzfrjxv3/arXiv22.pdf?dl=0">[pdf]</a>
               </LI>
               <p>
               <LI>
                <b> "Detecting Deep-Fake Videos from Aural and Oral Dynamics", </b>
                <BR>
                <b> S. Agarwal </b> and H. Farid,
                <BR>
                <em>Workshop on Media Forensics at CVPR, Online, 2021. </em> 
                <a href="https://www.dropbox.com/s/73vawhh4ifgvu0l/cvpr21a.pdf?dl=0">[pdf]</a>
             </LI>
             <p>
              <LI>
                  <b> "Detecting Deep-Fake Videos from Appearance and Behavior", </b>
                  <BR>
                  <b> S. Agarwal </b>, T. Gaaly, H. Farid, and S. Lim,
                  <BR>
                  <em>IEEE Workshop on Information Forensics and Security (WIFS), Online, 2020. </em> <a href="https://www.dropbox.com/s/qsuo31313og5ntq/WIFS20.pdf?dl=0">[pdf]</a>
               </LI>
              <p>
              <LI>
                  <b> "Detecting Deep-Fake Videos from Phoneme-Viseme Mismatches", </b>
                  <BR>
                  <b> S. Agarwal </b>, O. Fried and M. Agrawala, and H. Farid,
                  <BR>
                  <em>Workshop on Media Forensics at CVPR, Seattle, WA, 2020. </em> 
                  <a href="https://www.dropbox.com/s/5aypisdxfpzo9tr/cvprw20.pdf?dl=0">[pdf]</a>
               </LI>
               <p>
              <LI>
                  <b> "Protecting World Leaders Against Deep Fakes", </b>
                  <BR>
                  <b> S. Agarwal </b>, H. Farid, Y. Gu, M. He, K. Nagano, and H. Li,
                  <BR>
                  <em>Workshop on Media Forensics at CVPR, Long Beach, CA, 2019. </em> <a href="https://www.dropbox.com/s/gcxrao1aakreo5z/CVPRW19.pdf?dl=0">[pdf]</a> <a href="https://www.dropbox.com/s/udwfak8z4my2cac/cvprw19_poster.pdf?dl=0">[poster]</a> <a href="https://www.dropbox.com/s/a6bv8eopdd691hz/cvprw19.key?dl=0">[ppt]</a> 
               </LI><p>
              </ul>

            </LI>
          </ul>
        </td>
      <tr>

        <tr>
          <td  valign="top" align="left">
            <img src="projects/perception.png" border=0 height=250></a>
          </td>
          <td valign="top" align="left">
            <ul>
              <LI>
                <b> Human Perceptual Studies:  </b> 
                <BR>
                  In today’s digital world, facial images are used as a proof of identity at many venues –
                  from social media accounts to national identification documents. With recent advances in
                  techniques for face synthesis and manipulations, it is becoming easier to create fake identities
                  using synthetic faces. Here, we perform perceptual studies to determine how
                  well humans can detect original faces from synthetic and manipulated faces.
                <p>
                <ul>
                <LI>
                  <b> "Synthetic Faces: how perceptually convincing are they?", </b>
                  <BR>
                  S.J. Nightingale,  <b> S. Agarwal </b>, E. Harkonen, J. Lehtinen, and H. Farid,
                  <BR>
                  <em>Vision Sciences, 2021. </em> <a href="https://www.dropbox.com/s/0a3wtpxkuawp47d/vss21_poster.pdf?dl=0">[poster]</a>
                </LI><p>

                  <LI>
                    <b> "Perceptual and Computational Detection of Face Morphing", </b>
                    <BR>
                    S.J. Nightingale,  <b> S. Agarwal </b>, and H. Farid,
                    <BR>
                    <em>Vision Sciences, 2021. </em> <a href="https://www.dropbox.com/s/dpryi7pjv0qkp0k/JOV20.pdf?dl=0">[pdf]</a>
                  </LI><p>

                  <LI>
                    <b> "Can We Detect Face Morphing to Prevent Identity Theft?", </b>
                    <BR>
                    S.J. Nightingale,  <b> S. Agarwal </b>, and H. Farid,
                    <BR>
                    <em>Vision Sciences, 2020. </em> <a href="https://jov.arvojournals.org/article.aspx?articleid=2771438">[abstract]</a>
                  </LI>
              </LI>
            </ul>
          </td>
        <tr>

      <tr>
        <td  valign="top" align="left">
          <img src="projects/wifs17.jpg" border=0 height=180></a>
        </td>
        <td valign="top" align="left">
          <ul>
            <LI>
              <b> JPEG Dimples:  </b> 
              <BR>
              Although the basic steps for JPEG encoding remain the same across different JPEG encoders, there are many aspects to this compression scheme that varies with the specific design and implementation choices made by an encoder. These variations can be exploited by forensic techniques to reveal traces of manipulation in digital images. One such aspect is the choice of the mathematical operator used to convert Discrete Cosine Transform (DCT) coefficients from floating-point to integer values. The use of directed rounding (ceiling or floor) for this purpose can lead to a periodic artifact in the form of a single darker or brighter pixel – which we term as JPEG dimples – in 8X8 pixel block of the JPEG images. An analysis of thousands of different camera models reveals a widespread prevalence of this artifact in real world JPEG images. Local manipulations like content-aware fill, re-sampling, compositing etc. in an image can disrupt the periodicity of this artifact in the altered region. We have shown that this local absence of dimples in an image can be exploited for the purpose of photo forensics to detect a wide range of digital manipulations.
              <p>
              <ul>
              <LI>
                  <b> "Photo Forensics From Rounding Artifacts", </b>
                  <BR>
                    <b> S. Agarwal </b> and H. Farid,
                  <BR>
                  <em>ACM Workshop on Information Hiding and Multimedia Security, Denver CO, 2020. </em> <a href="https://www.dropbox.com/s/4tb5c1mwmkhpwmh/IHMMSec20.pdf?dl=0">[pdf]</a>
               </LI><p>
              <LI>
                  <b> "A JPEG Corner Artifact from Directed Rounding of DCT Coeffcients", </b>
                  <BR>
                    <b> S. Agarwal </b> and H. Farid,
                  <BR>
                  <em>TR2018-838, Department of Computer Science, Dartmouth College, 2018. </em> <a href="https://www.dropbox.com/s/xbbfmh7rm6xf9wr/tr18.pdf?dl=0">[pdf]</a>
               </LI><p>
              <LI>
                  <b> "Photo forensics from JPEG dimples", </b>
                  <BR>
                    <b> S. Agarwal </b> and H. Farid,
                  <BR>
                  <em>IEEE Workshop on Information Forensics and Security (WIFS), Rennes, 2017. </em> </em> <a href="https://www.dropbox.com/s/11qcsbxect4zcru/wifs17.pdf?dl=0">[pdf]</a> <a href="https://www.dropbox.com/s/cqawidxvsu4055w/wifs17.key?dl=0">[ppt]</a> 
              </LI><p>
              </ul>

            </LI>
          </ul>
        </td>
      <tr>
        <td  valign="top" align="left">
          <img src="projects/icassp18.jpg" border=0 height=180></a>
        </td>
        <td valign="top" align="left">
          <ul>
            <LI>
              <b> Rebroadcast Detection:  </b> 
              <BR>
              Many forensic techniques, including JPEG Dimples, look for clues of modifications in the camera properties of an original JPEG file. This assumption can, however, fail in the presence of a simple rebroadcast attack in which an image is manipulated and then re-imaged, thus preserving the underlying camera properties in the image. A rebroadcast image can be generated by: (1) photographing a printed copy of an image; (2) photographing a displayed image; (3) scanning a printed copy of an image; or (4) capturing a screen-grab of a displayed image. We have collected a large-scale dataset comprising all four types of rebroadcast images and evaluated the efficacy of three state-of-the-art techniques on our dataset. We also trained a deep convolutional neural network (CNN) to detect rebroadcast images and showed that our network significantly outperformed the previous state-of-the-art techniques for this task.
              <p>
              <ul>
              <LI>
                <b> "Rebroadcast Attacks: Defenses, Reattacks, and Redefenses", </b>
                <BR>
                  W. Fan, <b> S. Agarwal </b>, and H. Farid,
                <BR>
                <em>European Signal Processing Conference (EUSIPCO), Rome, Italy, IEEE, 2018. (to appear)</em> </em> <a href="https://www.dropbox.com/s/ew7t4e1ng0avgbx/eusipco18.pdf?dl=0">[pdf]</a> <a href="https://www.dropbox.com/s/5vks3gpljx9kect/eusipco18.key?dl=0">[ppt]</a>
              </LI><p>
              <LI>
                <b> "A Diverse Large-Scale Dataset for Evaluating Rebroadcast Attacks", </b>
                <BR>
                  <b> S. Agarwal </b>, W. Fan, and H. Farid,
                <BR>
                <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary, Alberta, Canada, 2018. </em> <a href="https://www.dropbox.com/s/b6mvnisge7a5jqj/icassp18.pdf?dl=0">[pdf]</a> <a href="https://www.dropbox.com/s/q63ihrd0vvceqav/icassp18.key?dl=0">[ppt]</a> <a href="https://drive.google.com/drive/folders/1az3B7jGnZ7ChpN92nOnK12qNNWLS7sEh?usp=sharing">[data]</a>
              </LI><p>
              </ul>
            </LI>
          </ul>
        </td>
      <tr>
        <td  valign="top" align="left">
          <img src="projects/spie17.jpg" border=0 height=180></a>
        </td>
        <td valign="top" align="left">
          <ul>
            <LI>
              <b> Deciphering Severely Degraded License Plates:  </b> 
              <BR>
              License plate images obtained from surveillance cameras are often noisy and of low resolution images, on the order of 20 pixels in width. These low quality images appear with frustrating frequency in many forensic investigations. The inability of human observers to decipher such low quality images, brings forth the need for computational tools to perform this task. With the help of a large-scale simulation we first showed that useful distinctive information remains even in images of resolution as low as 1.9 pixels per character. We have trained a CNN to extract this information and recognized characters on degraded real-world license plate images. 
            </LI>
            <ul>
            <LI>
              <b> "Forensic Reconstruction of Severely Degraded License Plates", </b>
              <BR>
                B. Lorch, <b> S. Agarwal </b>, and H. Farid,
              <BR>
              <em>IS&T Electronic Imaging, San Francisco, CA, 2019. </em> <a href="https://www.dropbox.com/s/nrvcnqezusfhgyn/ist19.pdf?dl=0">[pdf]</a>
          </LI><p>
            <LI>
              <b> "Deciphering Severely Degraded License Plates", </b>
              <BR>
                <b> S. Agarwal </b>, D. Tran, L. Torresani and H. Farid,
              <BR>
              <em>IS&T Electronic Imaging, San Francisco, CA, 2017. </em> <a href="https://www.dropbox.com/s/r7aeh6h4c9xw2bb/spie17.pdf?dl=0">[pdf]</a> <a href="https://www.dropbox.com/s/tslmbvz6ladf7g5/spie17_poster.pdf?dl=0">[poster]</a> <a href="https://www.dropbox.com/s/qxdqfjcnpbm8dp5/spie17.key?dl=0">[ppt]</a>
          </LI><p>
          </ul>
          </ul>
        </td>
    </table>

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
